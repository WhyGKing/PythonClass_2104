{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "universal-bridal",
   "metadata": {},
   "source": [
    "▣ swcodingschool's classroom\n",
    "Web Crawling\n",
    "1. requests 모듈\n",
    "파이썬으로 HTTP 호출하는 프로그램을 작성할 때 가장 많이 사용\n",
    "\n",
    "Requests document : https://docs.python-requests.org/en/master/\n",
    "\n",
    "사용전 설치 먼저...\n",
    "# requests 패키지 설치하고 import하기\n",
    "!pip install --upgrade pip\n",
    "!pip install requests\n",
    "API\n",
    "requests 라이브러리는 매우 직관적인 API를 제공한다. 어떤 방식(method)의 HTTP 요청을 하느냐에 따라서 해당하는 이름의 함수를 사용하면 된다.\n",
    "\n",
    "GET 방식: requests.get()\n",
    "POST 방식: requests.post()\n",
    "PUT 방식: requests.put()\n",
    "DELETE 방식: requests.delete()\n",
    "응답상태\n",
    "온라인 서비스를 HTTP로 호출하면 상태 코드를 응답받는다. 일반적으로 이 상태 코드를 보고 요청이 잘 처리되었는지 문제가 있는지 알 수 있다.\n",
    "\n",
    "상태 코드는 응답 객체의 status_code 속성을 통해 간단하게 얻을 수 있다.\n",
    "\n",
    "응답전문(response body/payload)\n",
    "요청이 정상적으로 처리된 경우, 응답전문에 요청한 데이터가 담겨져 온다. 응답전문은 크게 3가지 방식으로 읽어올 수 있다.\n",
    "\n",
    "첫번째, content 속성을 통해 바이너리 원문을 얻을 수 있다. 두번째, text속성을 통해 UTF-8로 인코딩된 문자열을 얻을 수 있다. 마지막으로, 응답 데이터가 JSON 포맷이라면 json() 함수를 통해 사전dictionary 객체를 얻을 수 있다.\n",
    "\n",
    "response = request.get(\"URL\")\n",
    "response.content\n",
    "response.text\n",
    "response.json()\n",
    "응답헤더\n",
    "응답에 대한 메타 데이터를 담고 있는 응답 헤더는 headers 속성을 통해 딕셔너리 형태로 얻을 수 있다.\n",
    "\n",
    "response.headers\n",
    "response.headers['Content-Type']\n",
    "요청 쿼리\n",
    "GET방식으로 HTTP요청을 ㅡ할 때는 쿼리 스트링(query string)을 이용, 응답받을 데이터를 필터링하는 경우가 많다.\n",
    "params 옵션을 사용하면 쿼리스트링을 딕셔너리 형태로 넘길 수 있다.\n",
    "\n",
    ">>> response = requests.get(\"https://jsonplaceholder.typicode.com/posts\", params={\"userId\": \"1\"})\n",
    ">>> [post[\"id\"] for post in response.json()]\n",
    "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# 네이버에 GET 요청하기\n",
    "import requests\n",
    "url = 'http://www.naver.com'\n",
    "response = requests.get(url)\n",
    "print(\"status code : \", response.status_code)\n",
    "# 응답코드로 200을 리턴. 정상적으로 값을 받았음을 확인.\n",
    "# 아래 명령어를 추가로 실행해서 응답받은 페이지 내용을 출력해 볼 수 있음\n",
    "# print(response.text)\n",
    "# POST 메소드를 사용하는 방법\n",
    "import requests\n",
    "url = 'http://www.naver.com'\n",
    "response = requests.post(url)\n",
    "print(\"status code : \", response.status_code)\n",
    "# 데이터 전송방법. 전달하기\n",
    "# 방법 1. 직접 파라미터를 넣어서 보내기 \n",
    "# 방법 2. dictionary 이용, params 를 사용 파라미터를 넣어서 보낼수 있다.\n",
    "# 방법 3. POST로 보내기\n",
    "# 방법 4. SSL 인증서를 사용하는 경우\n",
    "# 방법 5. 인증이 필요한 경우\n",
    "# 방법 1로 보내기 :\n",
    "import requests\n",
    "url = 'http://www.naver.com?a=bbb&b=123'\n",
    "\n",
    "print(\"status code : \", response.status_code)\n",
    "# 방법 2로 보내기\n",
    "import requests\n",
    "paramDict = {\"a\":\"bbb\", \"b\":123}\n",
    "url = \"http://www.naver.com\"\n",
    "response = requests.get(url, params=paramDict)\n",
    "print(\"staus code :\", response.status_code)\n",
    "# parameter 형태\n",
    "# 자료형 이름 = {\"param1\":value1, \"param2\":value2, ...}\n",
    "# 방법 3 post로 데이터 보내기\n",
    "# post로 보낼 때는 url주소에 파라미터를 넣지 않기 때문에 방법2와 같이 처리\n",
    "# 단, 다른 점은 get 메소드에서는 params 인자 사용. post 메소드에서는 data 인자 사용\n",
    "import requests\n",
    "paramDict = {\"a\":\"bbb\", \"b\":123}\n",
    "url = \"http://www.naver.com\"\n",
    "response = requests.post(url, data=paramDict)\n",
    "print(\"staus code :\", response.status_code)\n",
    "# 방법 4. SSL 인증서를 사용하는 경우\n",
    "# 보안 때문에 http보다 https를 많이 사용. 간혹 ssl때문에 오류가 발생. verify 옵션 추가\n",
    "import requests\n",
    "url = \"https://www.naver.com\"\n",
    "response = requests.post(url, verify=False)  # False의 경우 warning 발생. default값은 True.\n",
    "print(\"status code : \", response.status_code)\n",
    "# 방법 5. 인증이 필요한 경우\n",
    "# API를 사용할 때 KEY 토큰을 할당받아서 사용하기도 하지만, \n",
    "# id와 password를 이용해 인증하는 경우도 있음\n",
    "# 이때는 auth 옵션 사용\n",
    "import requests\n",
    "url = \"https://www.naver.com\"\n",
    "response=requests.post(url, auth=(\"id\", \"pass\"))\n",
    "print(\"status code : \", response.status_code)                     \n",
    "# 실습 따라하기\n",
    "# google 에 검색어 'python' 던지기\n",
    "import requests\n",
    "URL = 'https://www.google.com/search'\n",
    "\n",
    "#response = requests.get(URL)\n",
    "\n",
    "# get요청시 parameter 전달하기\n",
    "# params = {'param1': 'value1', 'param2': 'value'}\n",
    "# res = requests.get(URL, params=params)\n",
    "params = {'q' : 'python'}\n",
    "response = requests.get(URL, params=params)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.text)\n",
    "\n",
    "# 추가학습\n",
    "\"\"\"\n",
    "1. POST요청할 때 data 전달하기 : params대신 data라는 이름으로 준다.\n",
    "data = {'param1':'value1', 'param1':value2}\n",
    "res = requests.post(URL, data=data)\n",
    "\n",
    "2. 헤더추가, 쿠키 추가\n",
    "별도의 헤더 옵션을 추가하고자 할 때는 headers 옵션을, \n",
    "쿠키를 심어서 요청을 보내고 싶으면 cookies 옵션을 사용하면 된다\n",
    "\n",
    "headers = {'Content-Type': 'application/json; charset=utf-8'}\n",
    "cookies = {'session_id': 'sorryidontcare'}\n",
    "res = requests.get(URL, headers=headers, cookies=cookies)\n",
    "\n",
    "3. 응답객체(Reaponse)\n",
    "요청(request)을 보내면 응답(response)을 받는다. \n",
    "당연히 이 응답은 python 객체로 받는다. \n",
    "그리고 이 응답 객체는 많은 정보와 기능을 가지고 있다. \n",
    "ipython이나 jupyter notebook에서 <탭> 기능을 이용해서 직접 체험해보면 \n",
    "금방 파악이 가능하다.\n",
    "\"\"\"\n",
    "# 실습 따라하기\n",
    "print(response.request) # request객체에 접근 가능\n",
    "print(response.status_code) # 응답코드\n",
    "print(response.raise_for_status()) # 200 OK 코드가 아닌 경우 에러 발동\n",
    "#response.json()  # json response일 경우 딕셔너리 타입으로 바로 변환\n",
    "# 실습 따라하기 : 서울시 코로나 확진자 발생현황 스크래핑\n",
    "import requests\n",
    "URL = 'https://www.seoul.go.kr/coronaV/coronaStatus.do'\n",
    "response = requests.get(URL)\n",
    "html_data = response.text\n",
    "print(html_data)\n",
    "# 실습 따라하기 : 네이버 증권 \n",
    "import requests \n",
    "URL = 'https://finance.naver.com'\n",
    "#response = requests.get(URL)\n",
    "# 크롤러를 차단하였을 때, 브라우저를 이용한 직접 검색처럼 꾸미기\n",
    "header = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.128 Safari/537.36'}\n",
    "response = requests.get(URL, headers=header)\n",
    "\n",
    "html_data = response.text\n",
    "print(html_data)\n",
    "## requests 모듈 활용 샘플 코드 \n",
    "## 실제 사용시, 조건문을 이용 응답 코드에 따라 다른 로직으로 처리하도록 구성\n",
    "## 정상적으로 응답을 받을 경우와 잘못된 url 사용으로 4xx 코드 받을 경우\n",
    "import requests\n",
    "\n",
    "url = \"https://www.naver.com\"   # 정상적 url 요청\n",
    "#url = \"https://www.test.com\"   # 비정상적 url 요청\n",
    "rs = requests.post(url)\n",
    "\n",
    "rs_code = rs.status_code\n",
    "\n",
    "if int(rs_code) == 200:\n",
    "    print(\"페이지 데이터 정상 수신\")\n",
    "    page_data = rs.text\n",
    "    print(page_data)\n",
    "else:\n",
    "    print(rs_code,\"페이지 데이터 수신 실패\")   \n",
    "2. urllib 모듈\n",
    "urllib 모듈이란?\n",
    "파이썬의 표준 모듈로써 URL을 다루기 위한 모듈 패키지\n",
    "설치가 필요하지 않고, import urllib로 활용\n",
    "requests 모듈과 마찬가지로 URL과 관련된 여러가지 기능들을 제공\n",
    "https://docs.python.org/ko/3/library/urllib.html\n",
    "4가지의 하위 모듈\n",
    "request : url을 열고 읽기 위한 모듈(http요청). https://docs.python.org/ko/3.10/library/urllib.request.html\n",
    "parse : url구문 분석을 위한 모듈(url 해석 및 조작) https://docs.python.org/ko/3.10/library/urllib.parse.html\n",
    "error : request 모듈에서 발생하는 예외들을 포함하는 모듈 https://docs.python.org/ko/3.10/library/urllib.error.html\n",
    "robotparser : robots.txt 파일을 파싱하는 모듈 https://docs.python.org/ko/3.10/library/urllib.robotparser.html\n",
    "# urllib.request : Request()\n",
    "# urllib.request는 URL(Uniform Resource Locator)을 가져오기 위한 파이썬 모듈\n",
    "\n",
    "import urllib\n",
    "URL = 'https://introsjlee.pythonanywhere.com'\n",
    "request = urllib.request.Request(URL)\n",
    "print(request)\n",
    "print(request.full_url)\n",
    "print(request.type)\n",
    "print(request.host)\n",
    "# urllib.request : urlopen() \n",
    "# urlopen 함수의 형태로, 매우 간단한 인터페이스를 제공\n",
    "# 해당 url 열기, 응답데이터는 바이트 형식의 HTTPResponse 객체\n",
    "# request 객체 또는 URL을 직접 넣어도 가능\n",
    "import urllib\n",
    "URL = 'https://introsjlee.pythonanywhere.com'\n",
    "request = urllib.request.Request(URL)\n",
    "\n",
    "response1 = urllib.request.urlopen(request)\n",
    "response2 = urllib.request.urlopen(URL)\n",
    "print(response1)\n",
    "print(response2)\n",
    "print(response1.geturl())\n",
    "print(response1.getheaders())\n",
    "# 응용하여 사용해보기\n",
    "# rullib.request를 사용하는 가장 간단한 방법\n",
    "\n",
    "import urllib.request\n",
    "url = 'https://introsjlee.pythonanywhere.com'\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    html = response.read()\n",
    "print(html)\n",
    "# 응용하여 사용해보기 2\n",
    "# URL을 통해 리소스를 가져와서 임시 위치에 저장하려면, shutil.copyfileobj()와 tempfile.NamedTemporaryFile() 함수를 통해 수행\n",
    "import shutil\n",
    "import tempfile\n",
    "import urllib.request\n",
    "\n",
    "with urllib.request.urlopen('https://introsjlee.pythonanywhere.com') as response:\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "        shutil.copyfileobj(response, tmp_file)\n",
    "\n",
    "with open(tmp_file.name) as html:\n",
    "    pass\n",
    "\n",
    "print(html)\n",
    "# urllib.request.urlopen()\n",
    "# urllib.request에 포함되어 있으며, python3에서는 request까지 모두 import해야 에러가 발생하지 않는다.\n",
    "# 해당 url을 열고 데이터를 얻을 수 있는 함수와 클래스를 제공하며, http를 통해 웹 서버에 데이터를 얻는 데 많이 사용한다.\n",
    "# urlopen(url[,data [,timeout]])\n",
    "# url : 열고자 하는 url문자열, request클래스의 인스턴스가 포함된다.\n",
    "# data는 post방식으로 전송시에 서버로 업로드할 폼 데이터, url 인코딩 되어 있는 문자열\n",
    "# timeout : 내부에서 사용하는 모든 블로킹 연산에 사용할 타임아웃 시간\n",
    "#\n",
    "#\n",
    "# urlopen에서는 몇가지 메서드를 지원한다. 몇가지 정리하면...\n",
    "# urlopen().read([nbytes]) : nbyte의 데이터를 바이트 문자열로 읽음\n",
    "# urlopen().readline() : 한 줄의 텍스트를 바이트 문자열로 읽음\n",
    "# urlopen().info() : URL에 연관된 메타 정보를 담은 매핑 객체를 반환\n",
    "# urlopen().getcode() : HTTP 응답 코드를 정수로 반환( 200, 404 )\n",
    "# urlopen().close() : 연결을 닫는다\n",
    "#\n",
    "# urlopen().read() :\n",
    "# urlopen으로 연 객체를 읽고, 인자로 전달하는 숫자만큼 데이터를 읽음. 바이트 형식의 데이터\n",
    "# read([nbytes]) : nbyte의 데이터를 바이트 문자열로 읽음\n",
    "# readline() : 한 줄의 텍스트를 바이트 문자열로 읽어 리스트에 반환\n",
    "import urllib\n",
    "url = 'https://introsjlee.pythonanywhere.com'\n",
    "response = urllib.request.urlopen(url)\n",
    "byte_data = response.read() # 해당 url에 있는 html데이터를 바이트 문자열로 반환\n",
    "#byte_data = response.readline()\n",
    "print(byte_data)\n",
    "# info() 메서드를 쓰면 해당 URL의 메타 정보를 담은 매핑 객체를 반환\n",
    "print(response.info())\n",
    "# with 구문 활용, 이렇게도 표현 가능\n",
    "import urllib.request\n",
    "\n",
    "URL = 'https://introsjlee.pythonanywhere.com'\n",
    "req = urllib.request.Request(URL)\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    byte_data = response.read()\n",
    "print(byte_data)\n",
    "# urllib.request : decode()\n",
    "# 분석을 위해서 바이트 형식의 데이터를 원하는 형식으로 변환\n",
    "# 기본값으로 utf-8 사용\n",
    "import urllib\n",
    "URL = 'https://introsjlee.pythonanywhere.com'\n",
    "response = urllib.request.urlopen(URL)\n",
    "byte_data = response.read()\n",
    "text_data = byte_data.decode()\n",
    "print(text_data)\n",
    "# urlretrieve() \n",
    "# 웹상의이미지를 다운로드\n",
    "# 이미지 url 주소를 입력 후, 해당 주소 이미지를 다운로드해 저장\n",
    "import urllib.request\n",
    "img_src = 'https://img1.daumcdn.net/relay/cafe/original/?fname=http%3A%2F%2Fcfs8.blog.daum.net%2Fimage%2F33%2Fblog%2F2008%2F08%2F28%2F07%2F02%2F48b5cecb1de78%26filename%3D4%25EC%2595%2584%25EB%25A6%2584%25EB%258B%25A4%25EC%259A%25B4%25ED%2592%258D%25EA%25B2%25BD.jpg'\n",
    "save_name = 'test.png'\n",
    "urllib.request.urlretrieve(img_src, save_name)\n",
    "print(\"저장되었습니다.\")\n",
    "###\n",
    "# urllib.parse : urlparse()\n",
    "# url을 파싱하여 분석하기 위한 모듈\n",
    "# http뿐만 아니라 ftp, ssh, imap 등도 포함\n",
    "# url 형식 : 프로토콜://아이디:비밀번호@호스트:포트/하위경로?파라미터#색인\n",
    "# url을 6개로분리하여 반환\n",
    "import urllib\n",
    "parse = urllib.parse.urlparse('https://introsjlee.pythonanywhere.com/friends/detail/1')\n",
    "print(parse)\n",
    "print(parse[0])\n",
    "print(parse[1])\n",
    "print(parse[2])\n",
    "\n",
    "# urlsplit()\n",
    "# url을 5개로 분리하여 반환\n",
    "parse2 = urllib.parse.urlsplit('https://introsjlee.pythonanywhere.com/bookmark/detail/1')\n",
    "print(parse2)\n",
    "print(parse2[0])\n",
    "print(parse2[1])\n",
    "print(parse2[2])\n",
    "# urllib.parse : urlunparse(), urlunsplit()\n",
    "# 분리된 url을 다시 합침\n",
    "# 튜플(변경 불가능)로 반환되기 때문에 리스트(변경가능)로 변경하여 활용\n",
    "parse = list(parse)\n",
    "parse[1] = 'blog.daum.net'\n",
    "\n",
    "unparse = urllib.parse.urlunparse(parse)\n",
    "print(unparse)\n",
    "# parse_qs(), parse_qsl()\n",
    "# 쿼리를 파싱해서 각각 사전(qs) 및 리스트(qs1)로 반환\n",
    "# 쿼리를 변경하여 요청할 때 활용\n",
    "import urllib\n",
    "parse = urllib.parse.urlparse('https://www.naver.com?a=1&b=2&c=3&d=4')\n",
    "print(parse)\n",
    "print(parse.query)\n",
    "print(type(parse.query))\n",
    "#\n",
    "# \n",
    "qs = urllib.parse.parse_qs(parse.query)\n",
    "print(qs)\n",
    "print(type(qs))\n",
    "#\n",
    "#\n",
    "qs1 = urllib.parse.parse_qsl(parse.query)\n",
    "print(qs1)\n",
    "print(type(qs1))\n",
    "# urljoin(a,b)\n",
    "# a와 b url을 합쳐주는 기능\n",
    "# /에 따라 url주소가 달라지는 것 주의\n",
    "import urllib.parse\n",
    "url = 'https://naver.com/a/b'\n",
    "print(urllib.parse.urljoin(url, 'c'))\n",
    "print(urllib.parse.urljoin(url, '/c'))\n",
    "\n",
    "##\n",
    "url = 'https://naver.com/a/b/'\n",
    "print(urllib.parse.urljoin(url, 'c'))\n",
    "print(urllib.parse.urljoin(url, '/c'))\n",
    "# quote(), unquote()\n",
    "# 아스키 코드가 아닌 문자들을 퍼센트 인코딩을 변환\n",
    "# URL에 한글이 섞여 있을 때 발생하는 오류 해결을 위해 사용함\n",
    "import urllib\n",
    "url = 'https://search.naver.com/search.naver?query=파이썬'\n",
    "response = urllib.request.urlopen(url)  # UnicodeEncodeError 발생함\n",
    "#encoded = urllib.parse.quote('파이썬')\n",
    "#url = 'https://search.naver.com/search.naver?query=' + urllib.parse.quote('파이썬')\n",
    "response = urllib.request.urlopen(url)  # UnicodeEncodeError 발생함\n",
    "byte_data = response.read()\n",
    "text_data = byte_data.decode()\n",
    "print(text_data)  \n",
    "#print(urllib.parse.quote('파이썬'))\n",
    "# urllib 활용\n",
    "# 1. 홈페이지를 로컬에 파일로 저장\n",
    "# 파일 입출력 함수 활용(open, write, close )\n",
    "# 2. 헤더를 추가해서 모바일 페이지로 저장\n",
    "# request 함수에 헤더 인자에 값을 전달\n",
    "import urllib\n",
    "header = {'User-Agent' : 'Mozilla/5.0 (iPhone)'}\n",
    "request = urllib.request.Request(\"http://www.naver.com\", headers=header)\n",
    "data = urllib.request.urlopen(request).read()\n",
    "f = open(\"mobile.html\", \"wb\")\n",
    "f.write(data)\n",
    "f.close()\n",
    "# 파라미터를 변경해 여러 정보 가져오기\n",
    "# 파이썬 문법을 활용해 연관검색어 리스트 출력\n",
    "# 여러가지 검색어의 Naver 검색 결과 출력\n",
    "import urllib\n",
    "query_list = ['파이썬', '웹 크롤링', '빅데이터', 'python']\n",
    "url = 'https://search.naver.com/search.naver?query='\n",
    "for i in query_list:\n",
    "    new_url = url + urllib.parse.quote(i)\n",
    "    response = urllib.request.urlopen(new_url)\n",
    "    byte_data = response.read()\n",
    "    text_data = byte_data.decode()\n",
    "    print(text_data)\n",
    "import urllib\n",
    "URL = 'https://search.yahoo.com/search?p='\n",
    "URL = URL + urllib.parse.quote('파이썬')\n",
    "response_urllib = urllib.request.urlopen(URL)\n",
    "byte_data = response_urllib.read()\n",
    "text_data = byte_data.decode()\n",
    "print(text_data)\n",
    "# https://www.youtube.com/results?search_query=\n",
    "\n",
    "import urllib\n",
    "URL = 'https://www.youtube.com/results?search_query='\n",
    "URL = URL + urllib.parse.quote('파이썬')\n",
    "response_urllib = urllib.request.urlopen(URL)\n",
    "byte_data = response_urllib.read()\n",
    "text_data = byte_data.decode()\n",
    "print(text_data)\n",
    "BeautifulSoup 모듈\n",
    "홈페이지 내 데이터를 쉽게 추출할 수 있도록 도와주는 파이썬 외부 라이브러리\n",
    "웹 문서 내 수많은 HTML 태그들을 parser를 활용해 사용하기 편한 파이썬 객체로 만들어 제공\n",
    "웹 문서 구조를 알고 있다면, 아주 편하게 원하는 데이터를 뽑아 활용할 수 있음\n",
    "# 설치하기\n",
    "# !pip install bs4\n",
    "# 또는 \n",
    "# !pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "URL = 'http://www.naver.com'\n",
    "response_urllib = urllib.request.urlopen(URL)\n",
    "byte_data = response_urllib.read()\n",
    "text_data = byte_data.decode()\n",
    "\n",
    "soup = BeautifulSoup(text_data, 'html.parser')\n",
    "print(soup.prettify())\n",
    "BeautifulSoup 모듈 정의 : 기존 방식과의 차이점\n",
    "기존 방식 : 정규 표현식, 문자열 함수 등을 활용하여 홈페이지 텍스트 내 패턴을 분석하여 하나식 원한느 데이터를 찾아가는 형식\n",
    "BeuatifulSoup 방식 : html문서를 태그를 기반으로 구조화하여 태그로 원하는 데이터를 찾아가는 방식\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "req = requests.get('https://naver.com')\n",
    "html = req.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "result = soup.find_all('a', 'thumb', limit=5)\n",
    "news_list = []\n",
    "for i in result:\n",
    "    news_list.append(i.find(\"img\")[\"alt\"])\n",
    "print(news_list)\n",
    "\n",
    "print(soup.title)\n",
    "print(soup.title.name)\n",
    "print(soup.title.string)\n",
    "\n",
    "print(soup.img)\n",
    "# 네이버 영화 랭킹 가져오기\n",
    "# 1. 홈페이지 텍스트 가져오기\n",
    "# https://movie.naver.com/movie/sdb/rank/rmovie.nhn\n",
    "# 2. BeautifulSoup으로 파싱하기\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "URL = \"https://movie.naver.com/movie/sdb/rank/rmovie.nhn\"\n",
    "req = requests.get(URL)\n",
    "html = req.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "print(soup)\n",
    "# 텍스트에서 영화 랭킹 가져오기 Step 2.\n",
    "# 1. 텍스트에서 영화 랭킹 찾기\n",
    "# 2. 영화랭킹에 해당하는 부분의 태그 찾기\n",
    "movie_ranking_list = soup.find_all('div', 'tit3')\n",
    "for i in range(len(movie_ranking_list)):\n",
    "    print(\"{:2}위:{}\".format(i+1, movie_ranking_list[i].get_text().strip()))\n",
    "# BeautifulSoup 모듈을 활용하여 네이버 뉴스 페이지의 헤드라인 뉴스를 가져오는 \n",
    "# 코드를 작성하세요.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "URL = \"http://news.naver.com\"\n",
    "req = requests.get(URL)\n",
    "html = req.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "print(soup)\n",
    "head_news_list = []\n",
    "result_bit = soup.find_all('p', \"hdline_fick_kit\")\n",
    "for i in result_big:\n",
    "    head_news_list.append(i.get_text())\n",
    "result_small = soup.find_all('div', \"hdline_article_tit\")\n",
    "for i in result_small:\n",
    "    head_news_list.append(i.get_text().strip())\n",
    "print(head_news_list)\n",
    "Selenium\n",
    "# 설치하기\n",
    "# !pip install selenium\n",
    "#  셀레니움 web driver 설치\n",
    "# 크롭 웹드라이버 : https://chromedriver.chromium.org/downloads\n",
    "# \n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "path = \"C:/Users/dears/chromedriver/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get('https://www.naver.com')\n",
    "element = driver.find_element_by_id(\"themecast\")\n",
    "print(element)\n",
    "element = driver.find_element_by_class_name(\"sc_newscast\")\n",
    "print(element)\n",
    "element = driver.find_element_by_class_name(\"nav\") # 지도 찾아서 클릭해보기\n",
    "print(element)\n",
    "element.click()\n",
    "element = driver.find_element_by_id(\"query\")\n",
    "print(element)\n",
    "element.send_keys(\"웹크롤링\")\n",
    "#셀레이움 모듈을 활용하여 네이버 메인 페이지에서 사전 바로가기 링크를 클릭한 뒤, \n",
    "# 사전 페이지에서 파이썬 검색어를 입력 및 검색한 결과를 브라우저로 띄우는 코드를 작성하세요.\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "path = \"C:/Users/dears/chromedriver/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get('https://www.naver.com')\n",
    "\n",
    "element = driver.find_element_by_class_name(\"mn_dic\")\n",
    "print(element)\n",
    "\n",
    "element.click()\n",
    "element = driver.find_element_by_id(\"ac_input\")\n",
    "element.send_keys(\"python\")\n",
    "element = driver.find_element_by_class_name(\"btn_search\")\n",
    "element.click()\n",
    "# BeautifuSoup 모듈 응용\n",
    "# html의 DOM의 정의\n",
    "# 노드를 활용한 검색\n",
    "# BeautifulSoup 모듈의함수를 활용하여 노드를 기준으로 원하는 데이터 추출\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "URL = \"http://www.naver.com\"\n",
    "req = requests.get(URL)\n",
    "html = req.text\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "for child in contents.children:\n",
    "    print(child)\n",
    "img_tag = contents.find('img')\n",
    "print(img_tag)\n",
    "print(img_tag.parent)\n",
    "#find_next_sibling() 바로 다음 형제 노드를 검색\n",
    "#find_next_siblings() 모든 형제노드를 검색\n",
    "#find_previous_sibling()\n",
    "#find_previous_siblings()\n",
    "from bs4 import BeautifulSoup as bs\n",
    "URL = \"http://www.naver.com\"\n",
    "req = requests.get(URL)\n",
    "html = req.text\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contetns = soup.find('body')\n",
    "\n",
    "p_tag = bs.find(\"p\", class_=\"b\")\n",
    "print(p_tag)\n",
    "\n",
    "print(p_tag.find_next_sibling())\n",
    "print(p_tag.find_next_siblings())\n",
    "# selenium 모듈 응용\n",
    "# 폼양식 전송 실습\n",
    "# selenium + web driver를 활용하여 버튼을 클릭하거나 자바스크립트 삽입 등 특정 조건이 충족되어야만 접근 가능한 대이터를\n",
    "# 모듈을 활용하여 가져올 수 있음\n",
    "# 활용예제 : 폼 양식을 이용해 naver 로그인하기 실습\n",
    "import selenium\n",
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "path = \"C:/Users/dears/chromedriver/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get('https://www.naver.com')\n",
    "\n",
    "element = driver.find_element_by_class_name(\"link_login\")\n",
    "element.click()\n",
    "# 로그인 페이지가 뜨면 로그인 정보 입력\n",
    "id = 'yourid'\n",
    "#pw = 'password'\n",
    "pw = 'yourpassword'\n",
    "element=driver.find_element_by_id('id')\n",
    "element.send_keys(id)\n",
    "time.sleep(1)\n",
    "element=driver.find_element_by_id('pw')\n",
    "element.send_keys(pw)\n",
    "time.sleep(1)\n",
    "logbtn = driver.find_element_by_id(\"log.login\")\n",
    "logbtn.click()\n",
    "# 자동 로그인 방지를 위한 capchar 적용\n",
    "# 자동화된 크롤러 구성에 불편함\n",
    "# 자동 로그인 방지를 위한 capchar 적용\n",
    "# 자동화된 크롤러 구성에 불편함\n",
    "# 회피방법 : \n",
    "# 자바스크립트로 데이터 입력하기\n",
    "# 로그인폼의 id와 pwd요소에 직접 데이터 전달(send_keys보다 훨씬 안정적)\n",
    "\n",
    "import selenium\n",
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "path = \"C:/Users/dears/chromedriver/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get('https://www.naver.com')\n",
    "\n",
    "element = driver.find_element_by_class_name(\"link_login\")\n",
    "element.click()\n",
    "# 로그인 페이지가 뜨면 로그인 정보 입력\n",
    "id = 'yourid'\n",
    "#pw = 'password'\n",
    "pw = 'yourpassword'\n",
    "\n",
    "#*******************************\n",
    "driver.execute_script(\"document.getElementById('id').value=\\'\" + id +\"\\'\")\n",
    "time.sleep(1)\n",
    "driver.execute_script(\"document.getElementById('pw').value=\\'\" + pw +\"\\'\")\n",
    "time.sleep(1)\n",
    "#*******************************\n",
    "logbtn = driver.find_element_by_id(\"log.login\")\n",
    "logbtn.click()\n",
    "# BeautifulSoup + Selenium 모듈 응용\n",
    "# 메일 목록 가져오기 실습\n",
    "# BeautifulSoup 모듈 : 로그인이 필요하거나 어떠한 버튼 등을 클릭한 뒤 나오는 페이지의 정보들을 가져오기 어려움\n",
    "# selenium모듈 : 수많은 데이터들을 손쉽게 가져오기 어려움\n",
    "# ====> 두 모듈 함께 활용 : bs로 접근하기 어려운 페이지를 s으로 접속하여 bs로 분석\n",
    "\n",
    "# 실습\n",
    "# s을 활용한 네이버 로그인 및 메일 탭 클릭\n",
    "import selenium\n",
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "path = \"C:/Users/dears/chromedriver/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get('https://www.naver.com')\n",
    "\n",
    "element = driver.find_element_by_class_name(\"link_login\")\n",
    "element.click()\n",
    "# 로그인 페이지가 뜨면 로그인 정보 입력\n",
    "id = 'yourid'\n",
    "#pw = 'password'\n",
    "pw = 'yourpassword'\n",
    "\n",
    "#*******************************\n",
    "driver.execute_script(\"document.getElementById('id').value=\\'\" + id +\"\\'\")\n",
    "time.sleep(1)\n",
    "driver.execute_script(\"document.getElementById('pw').value=\\'\" + pw +\"\\'\")\n",
    "time.sleep(1)\n",
    "#*******************************\n",
    "logbtn = driver.find_element_by_id(\"log.login\")\n",
    "logbtn.click()\n",
    "\n",
    "\n",
    "# element = driver.find_element_by_class_name('btn_global')# 클래스네임 미확인\n",
    "# element.click()\n",
    "# time.sleep(1)\n",
    "element = driver.find_element_by_class_name('tab MY_TAB_MAIL')\n",
    "element.click()\n",
    "\n",
    "## 메일함 소스 가져오기\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")    # 셀레니움과 BS 의 연결\n",
    "# print(soup)\n",
    "mail_list = soup.select(\"div.mTitle\")\n",
    "cnt = 1\n",
    "for i in mail_list:\n",
    "    print(cnt)\n",
    "    print('메일제목 :', i.select_one('a').text)\n",
    "    print('메일내용 :', str(i.select_one('strong').text).split(':')[1])\n",
    "    print()\n",
    "    cnt +=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
